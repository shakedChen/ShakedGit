{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds:  [ 0  0  0  0  0  0  0  0  0 34]\n",
      "label:  [ 417  877  166 ... 3210   15   14]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open(r'C:\\Users\\Shaked Chen\\Desktop\\ShakedGit\\Coursera\\Natural_Language_Processing_in_TensorFlow\\sonnets_for_final_week_4.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIg2f1HBxqof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 6.9108 - accuracy: 0.0210\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 6.5000 - accuracy: 0.0224\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 6.3994 - accuracy: 0.0235s - loss: 6.400 - ETA: 0s - loss: 6.3989 - accuracy: \n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 6.2803 - accuracy: 0.0292\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 6.1891 - accuracy: 0.0358\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 6.1061 - accuracy: 0.0394\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 6.0331 - accuracy: 0.0393\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 5.9529 - accuracy: 0.0444\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 5.8654 - accuracy: 0.0502\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 5.7556 - accuracy: 0.0567\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 5.6410 - accuracy: 0.0631\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 5.5260 - accuracy: 0.0693\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 5.4106 - accuracy: 0.0742\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 5.2972 - accuracy: 0.0816\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 5.1906 - accuracy: 0.0872\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 5.0839 - accuracy: 0.0951\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 4.9791 - accuracy: 0.1010\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 4.8680 - accuracy: 0.1106\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 4.7643 - accuracy: 0.1199\n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 4.6614 - accuracy: 0.1306\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 4.5495 - accuracy: 0.1379\n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 4.4477 - accuracy: 0.1506\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 67s 4ms/sample - loss: 4.3389 - accuracy: 0.1638\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 4.2351 - accuracy: 0.1740\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 4.1271 - accuracy: 0.1903\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 4.0186 - accuracy: 0.2023\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 3.9201 - accuracy: 0.2156\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 3.8192 - accuracy: 0.2357\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 3.7246 - accuracy: 0.2565\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 3.6237 - accuracy: 0.2779\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 67s 4ms/sample - loss: 3.5292 - accuracy: 0.2982\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 3.4436 - accuracy: 0.3151\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 3.3411 - accuracy: 0.3344\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 3.2688 - accuracy: 0.3496\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 3.1778 - accuracy: 0.3715\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 3.0995 - accuracy: 0.3890\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 3.0264 - accuracy: 0.4091\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 2.9546 - accuracy: 0.4258\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 2.8740 - accuracy: 0.4430\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 2.8034 - accuracy: 0.4562\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 2.7421 - accuracy: 0.4702\n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 59s 4ms/sample - loss: 2.6831 - accuracy: 0.4833\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 2.6092 - accuracy: 0.5021\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 72s 5ms/sample - loss: 2.5482 - accuracy: 0.5166\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 2.5027 - accuracy: 0.5208\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 75s 5ms/sample - loss: 2.4479 - accuracy: 0.5387\n",
      "Epoch 47/100\n",
      "14496/15462 [===========================>..] - ETA: 4s - loss: 2.3924 - accuracy: 0.5499"
     ]
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fXTEO3GJ282"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP_Week4_Exercise_Shakespeare_Question.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
